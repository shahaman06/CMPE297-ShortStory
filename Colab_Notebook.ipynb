{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahaman06/CMPE297-ShortStory/blob/main/Colab_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Intial Files Setup\n",
        "\n",
        "!rm -r *\n",
        "!git clone \"https://github.com/shahaman06/CMPE297-ShortStory/\"\n",
        "!mv /content/CMPE297-ShortStory/* /content/\n",
        "!rm -r CMPE297-ShortStory README.md Colab_Notebook.ipynb"
      ],
      "metadata": {
        "id": "gLqlCV4x___F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APkMNevY0S95"
      },
      "outputs": [],
      "source": [
        "# import torch.nn as nn\n",
        "# import torchvision\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from torch.cuda import is_available\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torch.optim import Adam\n",
        "from torch.nn import BCELoss\n",
        "from torch import ones, randn, tensor\n",
        "from torchvision import transforms\n",
        "from modules import Discriminator, Generator\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "\n",
        "device = 'cuda' if is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob1QpK600S-I"
      },
      "outputs": [],
      "source": [
        "# Declaring GLOBAL VARS:\n",
        "BATCH_SIZE = 25\n",
        "IMG_SHAPE = 208\n",
        "CHANNELS = 3\n",
        "FEATURES = 16\n",
        "RANDOM_NOISE_SHAPE = 256"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PaintingDataset(Dataset):\n",
        "    def __init__(self, loc = os.path.join('/content/paintings/'), img_shape = 208):\n",
        "        self.loc = loc\n",
        "        self.img_shape = img_shape\n",
        "        self.paintings=[]\n",
        "        # removing invalid image files\n",
        "        for i in glob(os.path.join(loc+'*.jpg')):\n",
        "          img = cv2.imread(i)\n",
        "          if type(img) != type(None):\n",
        "            self.paintings.append(i)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paintings)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img = self.paintings[idx]\n",
        "        img = cv2.imread(img)\n",
        "        img = cv2.resize(img, (self.img_shape, self.img_shape))\n",
        "        img = np.moveaxis(img, -1, 0) # pytorch takes channel first images\n",
        "        img = tensor(img).float()\n",
        "        return img.to(device)"
      ],
      "metadata": {
        "id": "1Kihe7XgJayJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = PaintingDataset(img_shape = IMG_SHAPE,)\n",
        "loader = DataLoader(ds, shuffle = True, batch_size = BATCH_SIZE)\n",
        "batch = next(iter(loader))"
      ],
      "metadata": {
        "id": "BBFWb5Kv21et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRIDPUel0S-N"
      },
      "outputs": [],
      "source": [
        "disc = Discriminator().to(device)\n",
        "gen = Generator().to(device)\n",
        "\n",
        "lr = 0.0002\n",
        "epochs = 2\n",
        "\n",
        "optimD = Adam(disc.parameters(), lr = lr, betas = (0.5, 0.99))\n",
        "optimG = Adam(gen.parameters(), lr = lr, betas = (0.5, 0.99))\n",
        "\n",
        "criterion = BCELoss()\n",
        "\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "for data in tqdm(loader):\n",
        "    batch_size = data.shape[0]\n",
        "\n",
        "    ## Training discriminator\n",
        "\n",
        "    '''First feeding the real images'''\n",
        "    label = (ones(batch_size)* 0.9).to(device)\n",
        "    output = disc(data).reshape(-1)\n",
        "    lossD_real = criterion(output, label)\n",
        "\n",
        "    '''feeding generated images'''\n",
        "    label = (ones(batch_size) * 0.1).to(device)\n",
        "    rand_noise = randn((batch_size, RANDOM_NOISE_SHAPE, 1, 1)).to(device)\n",
        "    fake_image = gen(rand_noise)\n",
        "    output = disc(fake_image.detach()).reshape(-1)\n",
        "    lossD_fake = criterion(output, label)\n",
        "\n",
        "    '''Back propogating discriminator and updating weights'''\n",
        "    disc.zero_grad()\n",
        "    lossD = lossD_real + lossD_fake\n",
        "    lossD.backward()\n",
        "    optimD.step()\n",
        "\n",
        "    ## Training generator\n",
        "\n",
        "    label = ones(batch_size).to(device)\n",
        "    output = disc(fake_image).reshape(-1)\n",
        "    lossG = criterion(output, label)\n",
        "\n",
        "    '''Backpropogating'''\n",
        "    gen.zero_grad()\n",
        "    lossG.backward()\n",
        "    optimG.step()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}